{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fe0a98d4",
      "metadata": {
        "id": "fe0a98d4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hh5DVkUdTxz_",
        "outputId": "dce5aaa7-a9b6-4561-f907-ab27e5846a4d"
      },
      "id": "Hh5DVkUdTxz_",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a66ece41",
      "metadata": {
        "id": "a66ece41"
      },
      "outputs": [],
      "source": [
        "training=pd.read_csv('/content/drive/MyDrive/QC/training (1).csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8a0c874b",
      "metadata": {
        "id": "8a0c874b"
      },
      "outputs": [],
      "source": [
        "test=pd.read_csv('/content/drive/MyDrive/QC/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "26b5d819",
      "metadata": {
        "id": "26b5d819"
      },
      "outputs": [],
      "source": [
        "training_new=pd.read_csv('/content/drive/MyDrive/QC/training_new.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7c0e5857",
      "metadata": {
        "id": "7c0e5857"
      },
      "outputs": [],
      "source": [
        "test_new=pd.read_csv('/content/drive/MyDrive/QC/test_new.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yGX6lE66HXv8"
      },
      "id": "yGX6lE66HXv8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data processing:"
      ],
      "metadata": {
        "id": "Pz6XHasbHYUk"
      },
      "id": "Pz6XHasbHYUk"
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import training_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "import pandas as pd\n",
        "\n",
        "features_core=list(\"ABCDEFGHIJKLMN\")+['time']\n",
        "new_features=['O','P']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zKxfj9Frcg6g"
      },
      "id": "zKxfj9Frcg6g",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##combining features\n",
        "X_combiningd=pd.concat([training[features_core],training_new[new_features]],axis=1)\n",
        "\n",
        "##split 80% training,20% val for both targets\n",
        "X_training,X_val,y1_training,y1_val=training_test_split(X_combiningd,training['Y1'],test_size=0.2,random_state=1,shuffle=True)\n",
        "_,_,y2_training,y2_val=training_test_split(X_combiningd,training['Y2'],test_size=0.2,random_state=1,shuffle=True)"
      ],
      "metadata": {
        "id": "gIoFmMO5_yw6"
      },
      "id": "gIoFmMO5_yw6",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tune Y1 without K-Fold:**\n",
        "\n"
      ],
      "metadata": {
        "id": "XFRUAMoiW7cB"
      },
      "id": "XFRUAMoiW7cB"
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from lightgbm import early_stopping\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "##defi hyperparameters in one dict\n",
        "params_y1={\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'feature_fraction': 0.290108925533801, #27-done\n",
        "    'learning_rate': 0.03346736377247299, #33-done\n",
        "    'max_depth': 8,#done\n",
        "    'n_estimators': 800,#800 - done\n",
        "    'num_leaves': 54,#54-done\n",
        "    'lambda_l1': 0.8358897454113226,#83 -done\n",
        "    'lambda_l2': 0.23465766293184155,#23 -done\n",
        "    'verbosity': -1,\n",
        "    'random_state': 1\n",
        "}\n",
        "\n",
        "##training model with early stopping on val set\n",
        "model_y1=lgb.LGBMRegressor(**params_y1)\n",
        "\n",
        "model_y1.fit(\n",
        "    X_training,y1_training,\n",
        "    eval_set=[(X_val,y1_val)],\n",
        "    eval_metric='rmse',\n",
        "    callbacks=[early_stopping(stopping_rounds=50)],\n",
        "    #verbose=False\n",
        ")\n",
        "\n",
        "##predicting on validation set\n",
        "y1_val_pred=model_y1.predicting(X_val)\n",
        "val_r2_y1=r2_score(y1_val,y1_val_pred)\n",
        "print(f\"Y1 Validation R2: {val_r2_y1}\")\n",
        "\n",
        "##Log results to CSV and print\n",
        "import pandas as pd\n",
        "log_path='/content/drive/MyDrive/QC/y1_res.csv'\n",
        "\n",
        "##import pandas as pd\n",
        "from pandas.errors import EmptyDataError\n",
        "\n",
        "try:\n",
        "    log_df=pd.read_csv('/content/drive/MyDrive/QC/y1_res.csv')\n",
        "except (FileNotFoundError,EmptyDataError):\n",
        "    ##Handle file not existing or empty\n",
        "    log_df=pd.DataFrame()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "entry=params_y1.copy()\n",
        "entry['validation_r2']=val_r2_y1\n",
        "\n",
        "log_df=pd.concat([log_df,pd.DataFrame([entry])],ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "log_df.to_csv(log_path,index=False)\n",
        "\n",
        "print(\"Logged current parameters and validation R2 to y1_res.csv\")\n",
        "print(log_df.tail(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9jxUEXzhkQZ",
        "outputId": "b64eea69-8e98-47df-b046-8f4e8955377c"
      },
      "id": "V9jxUEXzhkQZ",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[499]\tvalid_0's rmse: 0.45012\n",
            "Y1 Validation R2: 0.7775851157118706\n",
            "Logged current parameters and validation R2 to y1_res.csv\n",
            "     objective metric  feature_fraction  learning_rate  max_depth  \\\n",
            "74  regression   rmse          0.290109       0.033467          8   \n",
            "75  regression   rmse          0.650000       0.010000         10   \n",
            "76  regression   rmse          0.290109       0.330000          8   \n",
            "77  regression   rmse          0.290109       0.033467          8   \n",
            "78  regression   rmse          0.290109       0.033467          8   \n",
            "\n",
            "    n_estimators  num_leaves  lambda_l1  lambda_l2  verbosity  random_state  \\\n",
            "74           800          54    0.83589   0.234658         -1            42   \n",
            "75           700          60    0.83589   0.234658         -1            42   \n",
            "76           800          54    0.83589   0.234658         -1            42   \n",
            "77           800          54    0.83589   0.234658         -1            42   \n",
            "78           800          54    0.83589   0.234658         -1            42   \n",
            "\n",
            "    validation_r2  \n",
            "74       0.777585  \n",
            "75       0.773438  \n",
            "76       0.766590  \n",
            "77       0.777585  \n",
            "78       0.777585  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tune Y2 without K-Fold:**"
      ],
      "metadata": {
        "id": "smgE8H2YXQ35"
      },
      "id": "smgE8H2YXQ35"
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from lightgbm import early_stopping\n",
        "from sklearn.metrics import r2_score\n",
        "import pandas as pd\n",
        "from pandas.errors import EmptyDataError\n",
        "\n",
        "##defi hyperparameters for Y2\n",
        "params_y2={\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'feature_fraction': 0.4,#0.5 -done\n",
        "    'learning_rate': 0.05, #0.05 -done\n",
        "    'max_depth': 7,#done -7\n",
        "    'n_estimators': 800,#done -800\n",
        "    'num_leaves': 47,#41 - done\n",
        "    #'lambda_l1' : 0.9358897454113226,#none\n",
        "    #'lambda_l2': 0.93465766293184155,#none\n",
        "    'verbosity': -1,\n",
        "    'random_state': 1\n",
        "}\n",
        "\n",
        "##traininging with early stopping\n",
        "model_y2=lgb.LGBMRegressor(**params_y2)\n",
        "\n",
        "model_y2.fit(\n",
        "    X_training,y2_training,\n",
        "    eval_set=[(X_val,y2_val)],\n",
        "    eval_metric='rmse',\n",
        "    callbacks=[early_stopping(stopping_rounds=50)],\n",
        "    ##verbose=False\n",
        ")\n",
        "\n",
        "##predicting on validation set\n",
        "y2_val_pred=model_y2.predicting(X_val)\n",
        "val_r2_y2=r2_score(y2_val,y2_val_pred)\n",
        "print(f\"Y2 Validation R2: {val_r2_y2}\")\n",
        "\n",
        "##log the results to CSV and print\n",
        "log_path_y2='/content/drive/MyDrive/QC/y2_res.csv'\n",
        "\n",
        "try:\n",
        "    log_df_y2=pd.read_csv(log_path_y2)\n",
        "except (FileNotFoundError,EmptyDataError):\n",
        "    log_df_y2=pd.DataFrame()\n",
        "\n",
        "entry_y2=params_y2.copy()\n",
        "entry_y2['validation_r2']=val_r2_y2\n",
        "\n",
        "log_df_y2=pd.concat([log_df_y2,pd.DataFrame([entry_y2])],ignore_index=True)\n",
        "\n",
        "log_df_y2.to_csv(log_path_y2,index=False)\n",
        "\n",
        "print(\"Logged current parameters and validation R2 to y2_res.csv\")\n",
        "print(log_df_y2.tail(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyEK4lx_1jII",
        "outputId": "97b289c8-c0e5-4207-a80b-bd7660a0521a"
      },
      "id": "YyEK4lx_1jII",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[592]\tvalid_0's rmse: 0.418602\n",
            "Y2 Validation R2: 0.8033225693098163\n",
            "Logged current parameters and validation R2 to y2_res.csv\n",
            "     objective metric  feature_fraction  learning_rate  max_depth  \\\n",
            "69  regression   rmse               0.5           0.05          7   \n",
            "70  regression   rmse               0.5           0.05          7   \n",
            "71  regression   rmse               0.5           0.05          7   \n",
            "72  regression   rmse               0.5           0.05          7   \n",
            "73  regression   rmse               0.4           0.05          7   \n",
            "\n",
            "    n_estimators  num_leaves  verbosity  random_state  validation_r2  \\\n",
            "69           800          47         -1            42       0.801339   \n",
            "70           800          47         -1            42       0.802802   \n",
            "71           800          47         -1            42       0.804228   \n",
            "72           800          47         -1            42       0.806805   \n",
            "73           800          47         -1            42       0.803323   \n",
            "\n",
            "    lambda_l1  lambda_l2  \n",
            "69        NaN   0.734658  \n",
            "70        NaN   0.834658  \n",
            "71        NaN   0.934658  \n",
            "72        NaN        NaN  \n",
            "73        NaN        NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ORZMDEPe1jE9"
      },
      "id": "ORZMDEPe1jE9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KhMoo9LB1jC4"
      },
      "id": "KhMoo9LB1jC4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION7**"
      ],
      "metadata": {
        "id": "O9uJL3Qj1jiP"
      },
      "id": "O9uJL3Qj1jiP"
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "\n",
        "features_base=list(\"ABCDEFGHIJKLMN\")+['time']\n",
        "new_features=['O','P']\n",
        "\n",
        "##combining all features for training and test\n",
        "X_training_full=pd.concat([training[features_base],training_new[new_features]],axis=1)\n",
        "y_training_full_Y1=training['Y1']\n",
        "y_training_full_Y2=training['Y2']\n",
        "X_test_full=pd.concat([test[features_base],test_new[new_features]],axis=1)\n",
        "\n",
        "##defi model for Y1 with given best params\n",
        "model_final_y1=lgb.LGBMRegressor(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    feature_fraction=0.2901089255338042,\n",
        "    learning_rate=0.03346736377247299,\n",
        "    max_depth=8,\n",
        "    n_estimators=800,\n",
        "    num_leaves=54,\n",
        "    lambda_l1=0.8358897454113226,\n",
        "    lambda_l2=0.23465766293184155,\n",
        "    verbosity=-1,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "##training model for Y1 on full data\n",
        "model_final_y1.fit(X_training_full,y_training_full_Y1)\n",
        "\n",
        "##defi model for Y2 with given best params\n",
        "model_final_y2=lgb.LGBMRegressor(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    feature_fraction=0.7,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=7,\n",
        "    n_estimators=500,\n",
        "    num_leaves=31,\n",
        "    verbosity=-1,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "\n",
        "##training model for Y2 on full data\n",
        "model_final_y2.fit(X_training_full,y_training_full_Y2)\n",
        "\n",
        "##predicting on test data\n",
        "preds_y1_test=model_final_y1.predicting(X_test_full)\n",
        "preds_y2_test=model_final_y2.predicting(X_test_full)\n",
        "\n",
        "##preparing the submission DataFrame\n",
        "submission_df=pd.DataFrame({\n",
        "    'id': test['id'].astype(int),\n",
        "    'Y1': preds_y1_test.astype(float),\n",
        "    'Y2': preds_y2_test.astype(float)\n",
        "})\n",
        "\n",
        "submission_df.to_csv('submission_best_params.csv',index=False)\n",
        "print(\"Submission saved as submission_best_params.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJEGbP1dfDy4",
        "outputId": "560c062c-f65c-4bb6-9133-806bbb912e8d"
      },
      "id": "pJEGbP1dfDy4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission saved as submission_best_params.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZicahjAZ53JJ"
      },
      "id": "ZicahjAZ53JJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`SUBMISSION 8`**"
      ],
      "metadata": {
        "id": "nB5EkhF253op"
      },
      "id": "nB5EkhF253op"
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "\n",
        "features_base=list(\"ABCDEFGHIJKLMN\")+['time']\n",
        "new_features=['O','P']\n",
        "\n",
        "##combining all features for training and test\n",
        "X_training_full=pd.concat([training[features_base],training_new[new_features]],axis=1)\n",
        "y_training_full_Y1=training['Y1']\n",
        "y_training_full_Y2=training['Y2']\n",
        "X_test_full=pd.concat([test[features_base],test_new[new_features]],axis=1)\n",
        "\n",
        "##defi model for Y1 with given best params\n",
        "model_final_y1=lgb.LGBMRegressor(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    feature_fraction=0.2901089255338042,\n",
        "    learning_rate=0.03346736377247299,\n",
        "    max_depth=8,\n",
        "    n_estimators=800,\n",
        "    num_leaves=54,\n",
        "    lambda_l1=0.8358897454113226,\n",
        "    lambda_l2=0.23465766293184155,\n",
        "    verbosity=-1,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "##training model for Y1 on full data\n",
        "model_final_y1.fit(X_training_full,y_training_full_Y1)\n",
        "\n",
        "##defi model for Y2 with updated params\n",
        "model_final_y2=lgb.LGBMRegressor(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    feature_fraction=0.5,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=7,\n",
        "    n_estimators=800,\n",
        "    num_leaves=47,\n",
        "    verbosity=-1,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "##training model for Y2 on full data\n",
        "model_final_y2.fit(X_training_full,y_training_full_Y2)\n",
        "\n",
        "##predicting on test data\n",
        "preds_y1_test=model_final_y1.predicting(X_test_full)\n",
        "preds_y2_test=model_final_y2.predicting(X_test_full)\n",
        "\n",
        "##preparing the submission DataFrame\n",
        "submission_df=pd.DataFrame({\n",
        "    'id': test['id'].astype(int),\n",
        "    'Y1': preds_y1_test.astype(float),\n",
        "    'Y2': preds_y2_test.astype(float)\n",
        "})\n",
        "\n",
        "submission_df.to_csv('submission7_9pm_21.csv',index=False)\n",
        "print(\"Submission saved as submission7.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09RqLklJ50eb",
        "outputId": "da055d97-9fde-49fb-ae13-4b9f03e2cf13"
      },
      "id": "09RqLklJ50eb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission saved as submission7.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION9**"
      ],
      "metadata": {
        "id": "pafq6utVJS3Q"
      },
      "id": "pafq6utVJS3Q"
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "\n",
        "features_base=list(\"ABCDEFGHIJKLMN\")+['time']\n",
        "new_features=['O','P']\n",
        "\n",
        "##combining all features for training and test\n",
        "X_training_full=pd.concat([training[features_base],training_new[new_features]],axis=1)\n",
        "y_training_full_Y1=training['Y1']\n",
        "y_training_full_Y2=training['Y2']\n",
        "X_test_full=pd.concat([test[features_base],test_new[new_features]],axis=1)\n",
        "\n",
        "##defi model for Y1 with given best params\n",
        "model_final_y1=lgb.LGBMRegressor(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    feature_fraction=0.65,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=10,\n",
        "    n_estimators=700,\n",
        "    num_leaves=60,\n",
        "    lambda_l1=0.8358897454113226,\n",
        "    lambda_l2=0.23465766293184155,\n",
        "    verbosity=-1,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "##training model for Y1 on full data\n",
        "model_final_y1.fit(X_training_full,y_training_full_Y1)\n",
        "\n",
        "##defi model for Y2 with updated params as\n",
        "model_final_y2=lgb.LGBMRegressor(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    feature_fraction=0.65,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=8,\n",
        "    n_estimators=700,\n",
        "    num_leaves=32,\n",
        "    verbosity=-1,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "##training model for Y2 on full data\n",
        "model_final_y2.fit(X_training_full,y_training_full_Y2)\n",
        "\n",
        "##predicting on test data\n",
        "preds_y1_test=model_final_y1.predicting(X_test_full)\n",
        "preds_y2_test=model_final_y2.predicting(X_test_full)\n",
        "\n",
        "##preparing the submission DataFrame\n",
        "submission_df=pd.DataFrame({\n",
        "    'id': test['id'].astype(int),\n",
        "    'Y1': preds_y1_test.astype(float),\n",
        "    'Y2': preds_y2_test.astype(float)\n",
        "})\n",
        "\n",
        "submission_df.to_csv('submission_check_with1st.csv',index=False)\n",
        "print(\"Submission saved as submission7.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxY3zzWz_DUw",
        "outputId": "728c1422-b9f6-43b3-b7d6-f5d0c4c8ecde"
      },
      "id": "wxY3zzWz_DUw",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission saved as submission7.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CHECK THE R score of SUBMISSION 9 on 20% validation set:**"
      ],
      "metadata": {
        "id": "vAT6teujclL_"
      },
      "id": "vAT6teujclL_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Y1:**"
      ],
      "metadata": {
        "id": "rozWp9lwc5nC"
      },
      "id": "rozWp9lwc5nC"
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from lightgbm import early_stopping\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "##defi hyperparameters in one dictionary\n",
        "params_y1={\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'feature_fraction': 0.2901089255338042, #27-done\n",
        "    'learning_rate': 0.03346736377247299, #33-done\n",
        "    'max_depth': 8,#8 - done\n",
        "    'n_estimators': 800,#800 - done\n",
        "    'num_leaves': 54,#54-done\n",
        "    'lambda_l1': 0.8358897454113226,#83 -done\n",
        "    'lambda_l2': 0.23465766293184155,#23 -done\n",
        "    'verbosity': -1,\n",
        "    'random_state': 1\n",
        "}\n",
        "\n",
        "##training model with early stopping on val set\n",
        "model_y1=lgb.LGBMRegressor(**params_y1)\n",
        "\n",
        "model_y1.fit(\n",
        "    X_training,y1_training,\n",
        "    eval_set=[(X_val,y1_val)],\n",
        "    eval_metric='rmse',\n",
        "    callbacks=[early_stopping(stopping_rounds=50)],\n",
        "    #verbose=False\n",
        ")\n",
        "\n",
        "##predicting on validation set\n",
        "y1_val_pred=model_y1.predicting(X_val)\n",
        "val_r2_y1=r2_score(y1_val,y1_val_pred)\n",
        "print(f\"Y1 Validation R2: {val_r2_y1}\")\n",
        "\n",
        "##log results to CSV and print\n",
        "import pandas as pd\n",
        "log_path='/content/drive/MyDrive/QC/y1_res.csv'\n",
        "\n",
        "##import pandas as pd\n",
        "from pandas.errors import EmptyDataError\n",
        "\n",
        "try:\n",
        "    log_df=pd.read_csv('/content/drive/MyDrive/QC/y1_res.csv')\n",
        "except (FileNotFoundError,EmptyDataError):\n",
        "    ##Handle file not existing or empty\n",
        "    log_df=pd.DataFrame()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "entry=params_y1.copy()\n",
        "entry['validation_r2']=val_r2_y1\n",
        "\n",
        "log_df=pd.concat([log_df,pd.DataFrame([entry])],ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "log_df.to_csv(log_path,index=False)\n",
        "\n",
        "print(\"Logged current parameters and validation R2 to y1_res.csv\")\n",
        "print(log_df.tail(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfgomN7xcxQZ",
        "outputId": "a9d09ffe-56b0-4f96-e7f3-79e541c12265"
      },
      "id": "qfgomN7xcxQZ",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[499]\tvalid_0's rmse: 0.45012\n",
            "Y1 Validation R2: 0.7775851157118706\n",
            "Logged current parameters and validation R2 to y1_res.csv\n",
            "     objective metric  feature_fraction  learning_rate  max_depth  \\\n",
            "73  regression   rmse          0.290109       0.033467          8   \n",
            "74  regression   rmse          0.290109       0.033467          8   \n",
            "75  regression   rmse          0.650000       0.010000         10   \n",
            "76  regression   rmse          0.290109       0.330000          8   \n",
            "77  regression   rmse          0.290109       0.033467          8   \n",
            "\n",
            "    n_estimators  num_leaves  lambda_l1  lambda_l2  verbosity  random_state  \\\n",
            "73           800          54    0.83589   0.934658         -1            42   \n",
            "74           800          54    0.83589   0.234658         -1            42   \n",
            "75           700          60    0.83589   0.234658         -1            42   \n",
            "76           800          54    0.83589   0.234658         -1            42   \n",
            "77           800          54    0.83589   0.234658         -1            42   \n",
            "\n",
            "    validation_r2  \n",
            "73       0.776151  \n",
            "74       0.777585  \n",
            "75       0.773438  \n",
            "76       0.766590  \n",
            "77       0.777585  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Y2:**"
      ],
      "metadata": {
        "id": "F55ZPPhIeSsc"
      },
      "id": "F55ZPPhIeSsc"
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from lightgbm import early_stopping\n",
        "from sklearn.metrics import r2_score\n",
        "import pandas as pd\n",
        "from pandas.errors import EmptyDataError\n",
        "\n",
        "##defi hyperparameters for Y2\n",
        "params_y2={\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'feature_fraction': 0.65,#0.5 -done\n",
        "    'learning_rate': 0.03, #0.05 -done\n",
        "    'max_depth': 8,#done -7\n",
        "    'n_estimators': 700,#done -800\n",
        "    'num_leaves': 32,#41 - done\n",
        "    #'lambda_l1' : 0.9358897454113226,#none\n",
        "    #'lambda_l2': 0.93465766293184155,#none\n",
        "    'verbosity': -1,\n",
        "    'random_state': 1\n",
        "}\n",
        "\n",
        "##traininging with early stopping\n",
        "model_y2=lgb.LGBMRegressor(**params_y2)\n",
        "\n",
        "model_y2.fit(\n",
        "    X_training,y2_training,\n",
        "    eval_set=[(X_val,y2_val)],\n",
        "    eval_metric='rmse',\n",
        "    callbacks=[early_stopping(stopping_rounds=50)],\n",
        "    ##verbose=False\n",
        ")\n",
        "\n",
        "##predicting on validation set\n",
        "y2_val_pred=model_y2.predicting(X_val)\n",
        "val_r2_y2=r2_score(y2_val,y2_val_pred)\n",
        "print(f\"Y2 Validation R2: {val_r2_y2}\")\n",
        "\n",
        "##Log results to CSV and print\n",
        "log_path_y2='/content/drive/MyDrive/QC/y2_res.csv'\n",
        "\n",
        "try:\n",
        "    log_df_y2=pd.read_csv(log_path_y2)\n",
        "except (FileNotFoundError,EmptyDataError):\n",
        "    log_df_y2=pd.DataFrame()\n",
        "\n",
        "\n",
        "entry_y2=params_y2.copy()\n",
        "entry_y2['validation_r2']=val_r2_y2\n",
        "\n",
        "log_df_y2=pd.concat([log_df_y2,pd.DataFrame([entry_y2])],ignore_index=True)\n",
        "\n",
        "log_df_y2.to_csv(log_path_y2,index=False)\n",
        "\n",
        "print(\"Logged current parameters and validation R2 to y2_res.csv\")\n",
        "print(log_df_y2.tail(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvMem9ZZeVjD",
        "outputId": "3d6a6275-e125-4929-f1c1-2559abf90e36"
      },
      "id": "FvMem9ZZeVjD",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\tvalid_0's rmse: 0.422082\n",
            "Y2 Validation R2: 0.8000387954671213\n",
            "Logged current parameters and validation R2 to y2_res.csv\n",
            "     objective metric  feature_fraction  learning_rate  max_depth  \\\n",
            "70  regression   rmse              0.50           0.05          7   \n",
            "71  regression   rmse              0.50           0.05          7   \n",
            "72  regression   rmse              0.50           0.05          7   \n",
            "73  regression   rmse              0.40           0.05          7   \n",
            "74  regression   rmse              0.65           0.03          8   \n",
            "\n",
            "    n_estimators  num_leaves  verbosity  random_state  validation_r2  \\\n",
            "70           800          47         -1            42       0.802802   \n",
            "71           800          47         -1            42       0.804228   \n",
            "72           800          47         -1            42       0.806805   \n",
            "73           800          47         -1            42       0.803323   \n",
            "74           700          32         -1            42       0.800039   \n",
            "\n",
            "    lambda_l1  lambda_l2  \n",
            "70        NaN   0.834658  \n",
            "71        NaN   0.934658  \n",
            "72        NaN        NaN  \n",
            "73        NaN        NaN  \n",
            "74        NaN        NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**\n",
        "\n",
        "*   The submission 7 and 8 overfitted on 20% validation data and thus they gave less R2 score compared to submission 9\n",
        "*   When submission 9's parameters are checked with that 20% validation data,I got the R2 score less than R2 score i got with submission 7 and 8,that means,i overfitted submission 7 and 8 on this 20% validation data.\n",
        "*   This 20% validation is taken with random split,but seed 1,and maintain same across all the notebook,or all the submission i made\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e2nCqWueflic"
      },
      "id": "e2nCqWueflic"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Steps:**\n",
        "*   I will check how much score does the best submission parameters - submission 9's parameters give on the 3 cross validation - the average of all 3 scores\n",
        "\n",
        "*   Then I will tune the hyperparameters starting from the submission 9's parameters ,such that they give better 3 fold validations score compared to the submission 9's 3-fold validation score\n",
        "*   List item\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZN-Zn4sWhKCl"
      },
      "id": "ZN-Zn4sWhKCl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MANUAL TUNING ON 3-FOLD FOR BEST PARAMETERS:**"
      ],
      "metadata": {
        "id": "ZM4T-NEkNwwb"
      },
      "id": "ZM4T-NEkNwwb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Y1:**"
      ],
      "metadata": {
        "id": "m2qanSH-L5m6"
      },
      "id": "m2qanSH-L5m6"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import r2_score\n",
        "from pandas.errors import EmptyDataError\n",
        "\n",
        "##defi features and target for Y1\n",
        "features_base=list(\"ABCDEFGHIJKLMN\")+['time']\n",
        "new_features=['O','P']\n",
        "X_y1=pd.concat([training[features_base],training_new[new_features]],axis=1)\n",
        "y_y1=training['Y1']\n",
        "\n",
        "#optuna_search_parameters(with op)(without cross fold):\n",
        "#Final : 0.754894-    Submitted - 0.5975\n",
        "##params={\n",
        "##    'objective': 'regression',\n",
        "##    'metric': 'rmse',\n",
        "##    'feature_fraction': 0.5524478320022036,\n",
        "##    'learning_rate': 0.019553385840773797,\n",
        "##    'max_depth': 9,\n",
        "##    'n_estimators': 1500,\n",
        "##    'num_leaves': 48,\n",
        "##    'verbosity': -1,\n",
        "##    'random_state': 1,\n",
        "##     'lambda_l1': 0.9263413222927331,\n",
        "##     'lambda_l2': 0.5416180534221455,\n",
        "##     'min_child_samples': 14,\n",
        "##     'bagging_fraction': 0.7054945561615795,\n",
        "##     'bagging_freq': 6\n",
        "##}\n",
        "\n",
        "#grid_search_parameters(with cross fold):\n",
        "###Final: 0.752910 - submitted - 0.710 - Ankur Submission ( i submitted with lamdas)\n",
        "##params={\n",
        "##    'objective': 'regression',\n",
        "##    'metric': 'rmse',\n",
        "##    'feature_fraction': 0.65,\n",
        "##    'learning_rate': 0.01,\n",
        "##    'max_depth': 10,\n",
        "##    'n_estimators': 700,\n",
        "##    'num_leaves': 60,\n",
        "##    #'lambda_l1': 0.8358897454113226,\n",
        "##    #'lambda_l2': 0.23465766293184155,\n",
        "##    'verbosity': -1,\n",
        "##    'random_state': 1\n",
        "##}\n",
        "\n",
        "\n",
        "###########################################################################################################################################\n",
        "#optuna parameters(no cross fold):\n",
        "#Final: 0.753753 - not submitted (y1_y2 file) - next this one - finetune and submit\n",
        "##params={\n",
        "##    'objective': 'regression',\n",
        "##    'metric': 'rmse',\n",
        "##    'feature_fraction': 0.5701089255338042,\n",
        "##    'learning_rate': 0.03346736377247299,\n",
        "##    'max_depth': 10,\n",
        "##    'n_estimators': 800,\n",
        "##    'num_leaves': 54,\n",
        "##    'lambda_l1': 0.8458897454113226,#53 - #84\n",
        "##    'lambda_l2': 0.43465766293184155,#43\n",
        "##    'verbosity': -1,\n",
        "##    'random_state': 1\n",
        "##}\n",
        "\n",
        "#passive/normal grid search(no cross fold) manual tuning:\n",
        "###Final:  0.747115 - submitted - submission 7 and 8\n",
        "##params={\n",
        "##    'objective': 'regression',\n",
        "##    'metric': 'rmse',\n",
        "##    'feature_fraction': 0.2901089255338042,\n",
        "##    'learning_rate': 0.03346736377247299,\n",
        "##    'max_depth': 8,\n",
        "##    'n_estimators': 800,\n",
        "##    'num_leaves': 54,\n",
        "##    'lambda_l1': 0.8358897454113226,#83 -done\n",
        "##    'lambda_l2': 0.23465766293184155,#23 -done\n",
        "##    'verbosity': -1,\n",
        "##    'random_state': 1\n",
        "##}\n",
        "\n",
        "#grid_search_parameters(with cross fold)-copied from there:\n",
        "#Final: 0.753379 - submitted 9\n",
        "params={\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'feature_fraction': 0.45,\n",
        "    'learning_rate': 0.01,\n",
        "    'max_depth': 10,\n",
        "    'n_estimators': 700,\n",
        "    'num_leaves': 60,\n",
        "    'lambda_l1': 0.735,#0.9358897454113226 #73 #8897454113226\n",
        "    'lambda_l2': 0.234,#0.23465766293184155 #23 #\n",
        "    'verbosity': -1,\n",
        "    'random_state': 1\n",
        "}\n",
        "\n",
        "##preparing the 3-fold cross validation (shuffle=False for time-series like behavior)\n",
        "kf=KFold(n_splits=3,shuffle=False)\n",
        "\n",
        "r2_scores=[]\n",
        "\n",
        "for fold,(training_idx,val_idx) in enumerate(kf.split(X_y1)):\n",
        "    print(f\"traininging fold {fold+1}\")\n",
        "    X_training_fold,X_val_fold=X_y1.iloc[training_idx],X_y1.iloc[val_idx]\n",
        "    y_training_fold,y_val_fold=y_y1.iloc[training_idx],y_y1.iloc[val_idx]\n",
        "\n",
        "    model=lgb.LGBMRegressor(**params)\n",
        "\n",
        "    model.fit(\n",
        "        X_training_fold,\n",
        "        y_training_fold,\n",
        "        eval_set=[(X_val_fold,y_val_fold)],\n",
        "        eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=50)],\n",
        "        #verbose=False\n",
        "    )\n",
        "\n",
        "    y_val_pred=model.predicting(X_val_fold)\n",
        "    fold_r2=r2_score(y_val_fold,y_val_pred)\n",
        "    print(f\"Fold {fold+1} R2: {fold_r2}\")\n",
        "    r2_scores.append(fold_r2)\n",
        "\n",
        "mean_r2=sum(r2_scores) / len(r2_scores)\n",
        "print(f\"Mean R2 after 3-fold CV: {mean_r2}\")\n",
        "\n",
        "##Log mean R2 and params to CSV\n",
        "log_path='/content/drive/MyDrive/QC/y1_cv_log.csv'\n",
        "\n",
        "try:\n",
        "    log_df=pd.read_csv(log_path)\n",
        "except (FileNotFoundError,EmptyDataError):\n",
        "    log_df=pd.DataFrame()\n",
        "\n",
        "##preparing the dict with param values & score\n",
        "log_entry=params.copy()\n",
        "log_entry['mean_cv_r2']=mean_r2\n",
        "\n",
        "log_df=pd.concat([log_df,pd.DataFrame([log_entry])],ignore_index=True)\n",
        "log_df.to_csv(log_path,index=False)\n",
        "\n",
        "print(f\"Logged CV results to {log_path}\")\n",
        "print(log_df.tail())  ##Latest entries in log\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXfpWZGcfpBR",
        "outputId": "5e4569a2-03ca-4019-8b86-fa9bfaef0f1b"
      },
      "id": "rXfpWZGcfpBR",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 1\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[348]\tvalid_0's rmse: 0.35533\n",
            "Fold 1 R2: 0.7819649697007693\n",
            "Training fold 2\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[273]\tvalid_0's rmse: 0.470236\n",
            "Fold 2 R2: 0.7477670493733117\n",
            "Training fold 3\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[645]\tvalid_0's rmse: 0.605082\n",
            "Fold 3 R2: 0.7328815620863225\n",
            "Mean R2 after 3-fold CV: 0.7542045270534677\n",
            "Logged CV results to /content/drive/MyDrive/QC/y1_cv_log.csv\n",
            "     objective metric  feature_fraction  learning_rate  max_depth  \\\n",
            "77  regression   rmse              0.55           0.01         10   \n",
            "78  regression   rmse              0.45           0.01         10   \n",
            "79  regression   rmse              0.45           0.01         10   \n",
            "80  regression   rmse              0.65           0.01          9   \n",
            "81  regression   rmse              0.45           0.01         10   \n",
            "\n",
            "    n_estimators  num_leaves  lambda_l1  lambda_l2  verbosity  random_state  \\\n",
            "77           700          60    0.73589   0.234658         -1            42   \n",
            "78           700          60    0.73589   0.234658         -1            42   \n",
            "79           700          50    0.73589   0.234658         -1            42   \n",
            "80           700          60    0.73500   0.234000         -1            42   \n",
            "81           700          60    0.73500   0.234000         -1            42   \n",
            "\n",
            "    mean_cv_r2  min_child_samples  bagging_fraction  bagging_freq  \n",
            "77    0.753880                NaN               NaN           NaN  \n",
            "78    0.754218                NaN               NaN           NaN  \n",
            "79    0.754205                NaN               NaN           NaN  \n",
            "80    0.753019                NaN               NaN           NaN  \n",
            "81    0.754205                NaN               NaN           NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Y2:**"
      ],
      "metadata": {
        "id": "O_0VLmmW3uOA"
      },
      "id": "O_0VLmmW3uOA"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import r2_score\n",
        "from pandas.errors import EmptyDataError\n",
        "\n",
        "##defi features and target for Y2,using old+new features\n",
        "features_base=list(\"ABCDEFGHIJKLMN\")+['time']\n",
        "new_features=['O','P']\n",
        "X_y2=pd.concat([training[features_base],training_new[new_features]],axis=1)\n",
        "y_y2=training['Y2']\n",
        "\n",
        "\n",
        "#passive/normal grid search(no cross fold) manual tuning:\n",
        "#Final : 0.648659    Submitted - Submission 8\n",
        "#after lamda : 0.655779\n",
        "##params={\n",
        "##    'objective': 'regression',\n",
        "##    'metric': 'rmse',\n",
        "##    'feature_fraction': 0.5,\n",
        "##    'learning_rate': 0.05,\n",
        "##    'max_depth': 7,\n",
        "##    'n_estimators': 800,\n",
        "##    'num_leaves': 47,\n",
        "##    'verbosity': -1,\n",
        "##    'random_state': 1,\n",
        "##    'lambda_l1': 0.9213114405513384,#92\n",
        "##    'lambda_l2': 0.4520962952151522  #45\n",
        "##}\n",
        "\n",
        "#passive/normal grid search(no cross fold) no manual tuning:\n",
        "#Final : 0.616942       Submitted - Submission 7\n",
        "##params={\n",
        "##    'objective': 'regression',\n",
        "##    'metric': 'rmse',\n",
        "##    'feature_fraction': 0.7,\n",
        "##    'learning_rate': 0.05,\n",
        "##    'max_depth': 7,\n",
        "##    'n_estimators': 500,\n",
        "##    'num_leaves': 31,\n",
        "##    'verbosity': -1,\n",
        "##    'random_state': 1\n",
        "##}\n",
        "\n",
        "#grid_search_parameters(with cross fold)-parameters copied: --used for deprecated\n",
        "#Final : 0.628903          Submitted - Submission 9\n",
        "params={\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'feature_fraction': 0.45,\n",
        "    'learning_rate': 0.03,\n",
        "    'max_depth': 9,\n",
        "    'n_estimators': 700,#700\n",
        "    'num_leaves': 92,#32-#72\n",
        "    'verbosity': -1,\n",
        "    'random_state': 1,\n",
        "    'lambda_l1': 0.9213114405513384,\n",
        "    'lambda_l2': 0.6520962952151522\n",
        "}\n",
        "\n",
        "##########################################################################################################################################\n",
        "\n",
        "#optuna_search_parameters(with op)(without cross fold):\n",
        "#Final : 0.635082    Submitted - 0.5975 score on leaderboard\n",
        "##params={\n",
        "##    'objective': 'regression',\n",
        "##    'metric': 'rmse',\n",
        "##    'feature_fraction': 0.6086455125133671,\n",
        "##    'learning_rate': 0.024435377516833785,\n",
        "##    'max_depth': 9,\n",
        "##    'n_estimators': 1500,\n",
        "##    'num_leaves': 43,\n",
        "##    'verbosity': -1,\n",
        "##    'random_state': 1,\n",
        "##    'lambda_l1': 0.6213114405513384,\n",
        "##    'lambda_l2': 0.6520962952151522,\n",
        "##    'min_child_samples': 13,\n",
        "##    'bagging_fraction': 0.9278603731265997,\n",
        "##    'bagging_freq': 7\n",
        "##}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##preparing the 3-fold cross validation (shuffle=False)\n",
        "kf=KFold(n_splits=3,shuffle=False)\n",
        "\n",
        "r2_scores=[]\n",
        "\n",
        "for fold,(training_idx,val_idx) in enumerate(kf.split(X_y2)):\n",
        "    print(f\"traininging fold {fold+1}\")\n",
        "    X_training_fold,X_val_fold=X_y2.iloc[training_idx],X_y2.iloc[val_idx]\n",
        "    y_training_fold,y_val_fold=y_y2.iloc[training_idx],y_y2.iloc[val_idx]\n",
        "\n",
        "    model=lgb.LGBMRegressor(**params)\n",
        "\n",
        "    model.fit(\n",
        "        X_training_fold,\n",
        "        y_training_fold,\n",
        "        eval_set=[(X_val_fold,y_val_fold)],\n",
        "        eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=50)],\n",
        "        #verbose=False\n",
        "    )\n",
        "\n",
        "    y_val_pred=model.predicting(X_val_fold)\n",
        "    fold_r2=r2_score(y_val_fold,y_val_pred)\n",
        "    print(f\"Fold {fold+1} R2: {fold_r2}\")\n",
        "    r2_scores.append(fold_r2)\n",
        "\n",
        "mean_r2=sum(r2_scores) / len(r2_scores)\n",
        "print(f\"Mean R2 after 3-fold CV for Y2: {mean_r2}\")\n",
        "\n",
        "##Log mean R2 and params - CSV\n",
        "log_path='/content/drive/MyDrive/QC/y2_cv_log.csv'\n",
        "\n",
        "try:\n",
        "    log_df=pd.read_csv(log_path)\n",
        "except (FileNotFoundError,EmptyDataError):\n",
        "    log_df=pd.DataFrame()\n",
        "\n",
        "##preparing the dict with param values & score\n",
        "log_entry=params.copy()\n",
        "log_entry['mean_cv_r2']=mean_r2\n",
        "\n",
        "log_df=pd.concat([log_df,pd.DataFrame([log_entry])],ignore_index=True)\n",
        "log_df.to_csv(log_path,index=False)\n",
        "\n",
        "print(f\"Logged CV results to {log_path}\")\n",
        "print(log_df.tail())  ##Latest entries in log\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibpwpSLq3zg_",
        "outputId": "1422c8ef-4a8e-4229-d283-15a21a3df8d4"
      },
      "id": "ibpwpSLq3zg_",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 1\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[104]\tvalid_0's rmse: 0.51172\n",
            "Fold 1 R2: 0.6637336794201056\n",
            "Training fold 2\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[47]\tvalid_0's rmse: 0.443454\n",
            "Fold 2 R2: 0.6657167581597498\n",
            "Training fold 3\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[205]\tvalid_0's rmse: 0.643169\n",
            "Fold 3 R2: 0.6504824728996081\n",
            "Mean R2 after 3-fold CV for Y2: 0.6599776368264879\n",
            "Logged CV results to /content/drive/MyDrive/QC/y2_cv_log.csv\n",
            "      objective metric  feature_fraction  learning_rate  max_depth  \\\n",
            "102  regression   rmse              0.55           0.03          9   \n",
            "103  regression   rmse              0.45           0.03          9   \n",
            "104  regression   rmse              0.40           0.03          9   \n",
            "105  regression   rmse              0.42           0.03          9   \n",
            "106  regression   rmse              0.45           0.03          9   \n",
            "\n",
            "     n_estimators  num_leaves  verbosity  random_state  mean_cv_r2  lambda_l1  \\\n",
            "102           700          92         -1            42    0.658752   0.921311   \n",
            "103           700          92         -1            42    0.659978   0.921311   \n",
            "104           700          92         -1            42    0.658623   0.921311   \n",
            "105           700          92         -1            42    0.658623   0.921311   \n",
            "106           700          92         -1            42    0.659978   0.921311   \n",
            "\n",
            "     lambda_l2  min_child_samples  bagging_fraction  bagging_freq  \n",
            "102   0.652096                NaN               NaN           NaN  \n",
            "103   0.652096                NaN               NaN           NaN  \n",
            "104   0.652096                NaN               NaN           NaN  \n",
            "105   0.652096                NaN               NaN           NaN  \n",
            "106   0.652096                NaN               NaN           NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import r2_score\n",
        "from pandas.errors import EmptyDataError\n",
        "\n",
        "features_base=list(\"ABCDEFGHIJKLMN\")+['time']\n",
        "new_features=['O','P']\n",
        "X_y2=pd.concat([training[features_base],training_new[new_features]],axis=1)\n",
        "y_y2=training['Y2']\n",
        "\n",
        "base_params={\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'feature_fraction': 0.45,\n",
        "    'max_depth': 9,\n",
        "    'n_estimators': 700,\n",
        "    'num_leaves': 92,\n",
        "    'verbosity': -1,\n",
        "    'random_state': 1,\n",
        "    'lambda_l1': 0.9213114405513384,\n",
        "    'lambda_l2': 0.6520962952151522\n",
        "}\n",
        "\n",
        "kf=KFold(n_splits=3,shuffle=False)\n",
        "log_path='/content/drive/MyDrive/QC/y2_cv_log.csv'\n",
        "\n",
        "try:\n",
        "    log_df=pd.read_csv(log_path)\n",
        "except (FileNotFoundError,EmptyDataError):\n",
        "    log_df=pd.DataFrame()\n",
        "\n",
        "results=[]\n",
        "\n",
        "for lr in [0.005,0.01,0.02,0.03,0.04,0.05,0.07,0.1]:\n",
        "    params=base_params.copy()\n",
        "    params['learning_rate']=lr\n",
        "    r2_scores=[]\n",
        "    for training_idx,val_idx in kf.split(X_y2):\n",
        "        X_training_fold,X_val_fold=X_y2.iloc[training_idx],X_y2.iloc[val_idx]\n",
        "        y_training_fold,y_val_fold=y_y2.iloc[training_idx],y_y2.iloc[val_idx]\n",
        "        model=lgb.LGBMRegressor(**params)\n",
        "        model.fit(\n",
        "            X_training_fold,\n",
        "            y_training_fold,\n",
        "            eval_set=[(X_val_fold,y_val_fold)],\n",
        "            eval_metric='rmse',\n",
        "            callbacks=[lgb.early_stopping(stopping_rounds=50)],\n",
        "        )\n",
        "        y_val_pred=model.predicting(X_val_fold)\n",
        "        r2_scores.append(r2_score(y_val_fold,y_val_pred))\n",
        "    mean_r2=sum(r2_scores) / len(r2_scores)\n",
        "    log_entry=params.copy()\n",
        "    log_entry['mean_cv_r2']=mean_r2\n",
        "    results.append(log_entry)\n",
        "\n",
        "log_df=pd.concat([log_df,pd.DataFrame(results)],ignore_index=True)\n",
        "log_df.to_csv(log_path,index=False)\n",
        "\n",
        "best_entry=max(results,key=lambda x: x['mean_cv_r2'])\n",
        "print(f\"Best learning rate: {best_entry['learning_rate']} with mean R2={best_entry['mean_cv_r2']}\")\n",
        "print(log_df.tail())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeuSJ1QPHbjQ",
        "outputId": "db25fbfc-5f44-4fd0-c003-69054b53dcac"
      },
      "id": "GeuSJ1QPHbjQ",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[458]\tvalid_0's rmse: 0.524316\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[285]\tvalid_0's rmse: 0.452472\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[699]\tvalid_0's rmse: 0.656654\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[238]\tvalid_0's rmse: 0.522455\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[136]\tvalid_0's rmse: 0.450373\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[634]\tvalid_0's rmse: 0.64237\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[157]\tvalid_0's rmse: 0.508741\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[70]\tvalid_0's rmse: 0.447881\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[407]\tvalid_0's rmse: 0.640836\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[104]\tvalid_0's rmse: 0.51172\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[47]\tvalid_0's rmse: 0.443454\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[205]\tvalid_0's rmse: 0.643169\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[74]\tvalid_0's rmse: 0.511543\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[32]\tvalid_0's rmse: 0.44278\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[216]\tvalid_0's rmse: 0.646765\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[62]\tvalid_0's rmse: 0.518842\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[29]\tvalid_0's rmse: 0.439476\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[154]\tvalid_0's rmse: 0.655585\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[46]\tvalid_0's rmse: 0.520521\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[18]\tvalid_0's rmse: 0.439826\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[171]\tvalid_0's rmse: 0.644394\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[21]\tvalid_0's rmse: 0.526015\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[14]\tvalid_0's rmse: 0.436847\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[79]\tvalid_0's rmse: 0.647897\n",
            "Best learning rate: 0.03 with mean R2 = 0.6599776368264879\n",
            "      objective metric  feature_fraction  learning_rate  max_depth  \\\n",
            "110  regression   rmse              0.45           0.03          9   \n",
            "111  regression   rmse              0.45           0.04          9   \n",
            "112  regression   rmse              0.45           0.05          9   \n",
            "113  regression   rmse              0.45           0.07          9   \n",
            "114  regression   rmse              0.45           0.10          9   \n",
            "\n",
            "     n_estimators  num_leaves  verbosity  random_state  mean_cv_r2  lambda_l1  \\\n",
            "110           700          92         -1            42    0.659978   0.921311   \n",
            "111           700          92         -1            42    0.659087   0.921311   \n",
            "112           700          92         -1            42    0.654284   0.921311   \n",
            "113           700          92         -1            42    0.657460   0.921311   \n",
            "114           700          92         -1            42    0.655204   0.921311   \n",
            "\n",
            "     lambda_l2  min_child_samples  bagging_fraction  bagging_freq  \n",
            "110   0.652096                NaN               NaN           NaN  \n",
            "111   0.652096                NaN               NaN           NaN  \n",
            "112   0.652096                NaN               NaN           NaN  \n",
            "113   0.652096                NaN               NaN           NaN  \n",
            "114   0.652096                NaN               NaN           NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONCLUSION:\n",
        "DONT USE:\n",
        "\n",
        "##     'min_child_samples': 14,\n",
        "##     'bagging_fraction': 0.7054945561615795,\n",
        "####    'bagging_freq': 6"
      ],
      "metadata": {
        "id": "-vidINnZHqym"
      },
      "id": "-vidINnZHqym"
    },
    {
      "cell_type": "code",
      "source": [
        "#NEW SUBMISSION\n",
        "    ##'lambda_l1': 0.9213114405513384,\n",
        "    ##'lambda_l2': 0.6520962952151522"
      ],
      "metadata": {
        "id": "jSHMESZ3jhNC"
      },
      "id": "jSHMESZ3jhNC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION DEPRECATED**"
      ],
      "metadata": {
        "id": "yTGVKhk08MOZ"
      },
      "id": "yTGVKhk08MOZ"
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "\n",
        "features_base=list(\"ABCDEFGHIJKLMN\")+['time']\n",
        "new_features=['O','P']\n",
        "\n",
        "##combining all features for training and test\n",
        "X_training_full=pd.concat([training[features_base],training_new[new_features]],axis=1)\n",
        "y_training_full_Y1=training['Y1']\n",
        "y_training_full_Y2=training['Y2']\n",
        "X_test_full=pd.concat([test[features_base],test_new[new_features]],axis=1)\n",
        "\n",
        "##defi model for Y1 with given best params\n",
        "model_final_y1=lgb.LGBMRegressor(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    feature_fraction=0.65,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=10,\n",
        "    n_estimators=700,\n",
        "    num_leaves=60,\n",
        "    lambda_l1=0.7358897454113226,\n",
        "    lambda_l2=0.23465766293184155,\n",
        "    verbosity=-1,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "##training model for Y1 on full data\n",
        "model_final_y1.fit(X_training_full,y_training_full_Y1)\n",
        "\n",
        "##defi model for Y2\n",
        "model_final_y2=lgb.LGBMRegressor(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    feature_fraction=0.65,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=8,\n",
        "    n_estimators=700,\n",
        "    num_leaves=32,\n",
        "    lambda_l1= 0.9213114405513384,\n",
        "    lambda_l2= 0.6520962952151522,\n",
        "    verbosity=-1,\n",
        "    random_state=202\n",
        ")\n",
        "#grid_search_parameters(with cross fold)-parameters copied: --used for deprecated\n",
        "#Final : 0.628903          Submitted - Submission 9\n",
        "##params={\n",
        "##    'objective': 'regression',\n",
        "##    'metric': 'rmse',\n",
        "##    'feature_fraction': 0.65,\n",
        "##    'learning_rate': 0.03,\n",
        "##    'max_depth': 8,\n",
        "##    'n_estimators': 700,\n",
        "##    'num_leaves': 32,\n",
        "##    'verbosity': -1,\n",
        "##    'random_state': 1,\n",
        "##    'lambda_l1': 0.9213114405513384,\n",
        "##    'lambda_l2': 0.6520962952151522\n",
        "##}\n",
        "\n",
        "\n",
        "##training model for Y2 on full data\n",
        "model_final_y2.fit(X_training_full,y_training_full_Y2)\n",
        "\n",
        "##predicting on test data\n",
        "preds_y1_test=model_final_y1.predicting(X_test_full)\n",
        "preds_y2_test=model_final_y2.predicting(X_test_full)\n",
        "\n",
        "##preparing the submission DataFrame\n",
        "submission_df=pd.DataFrame({\n",
        "    'id': test['id'].astype(int),\n",
        "    'Y1': preds_y1_test.astype(float),\n",
        "    'Y2': preds_y2_test.astype(float)\n",
        "})\n",
        "\n",
        "submission_df.to_csv('submission_enhanced_with_seed_l1_tuned.csv',index=False)\n",
        "print(\"Submission saved bro.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmuEj5mAjzZI",
        "outputId": "7a445cb8-a306-47da-96b8-028e5416c77d"
      },
      "id": "MmuEj5mAjzZI",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission saved bro.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NORMAL_GRIDSEARCH_vs_SUBMISSION7"
      ],
      "metadata": {
        "id": "76Pw27NQIq4E"
      },
      "id": "76Pw27NQIq4E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION_BEST_OF_ALL**"
      ],
      "metadata": {
        "id": "kNE9HM07F7KM"
      },
      "id": "kNE9HM07F7KM"
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "\n",
        "features_base=list(\"ABCDEFGHIJKLMN\")+['time']\n",
        "new_features=['O','P']\n",
        "\n",
        "##combining all features for training and test\n",
        "X_training_full=pd.concat([training[features_base],training_new[new_features]],axis=1)\n",
        "y_training_full_Y1=training['Y1']\n",
        "y_training_full_Y2=training['Y2']\n",
        "X_test_full=pd.concat([test[features_base],test_new[new_features]],axis=1)\n",
        "\n",
        "##defi model for Y1 with given best params\n",
        "model_final_y1=lgb.LGBMRegressor( ##not changed yet\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    feature_fraction=0.5701089255338042,\n",
        "    learning_rate=0.03346736377247299,\n",
        "    max_depth=10,\n",
        "    n_estimators=800,\n",
        "    num_leaves=54,\n",
        "    lambda_l1=0.8458897454113226,\n",
        "    lambda_l2=0.43465766293184155,\n",
        "    verbosity=-1,\n",
        "    random_state=1\n",
        ")\n",
        "##params={\n",
        "##    'objective': 'regression',\n",
        "##    'metric': 'rmse',\n",
        "##    'feature_fraction': 0.5701089255338042,\n",
        "##    'learning_rate': 0.03346736377247299,\n",
        "##    'max_depth': 10,\n",
        "##    'n_estimators': 800,\n",
        "##    'num_leaves': 54,\n",
        "##    'lambda_l1': 0.8458897454113226,#53 - #84\n",
        "##    'lambda_l2': 0.43465766293184155,#43\n",
        "##    'verbosity': -1,\n",
        "##    'random_state': 1\n",
        "##}\n",
        "\n",
        "##training model for Y1 on full data\n",
        "model_final_y1.fit(X_training_full,y_training_full_Y1)\n",
        "\n",
        "##defi model for Y2 with updated params\n",
        "model_final_y2=lgb.LGBMRegressor(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    feature_fraction=0.5,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=7,\n",
        "    n_estimators=800,\n",
        "    num_leaves=47,\n",
        "    lambda_l1= 0.9213114405513384,\n",
        "    lambda_l2= 0.4520962952151522,\n",
        "    random_state=1,\n",
        "    verbosity =-1,\n",
        ")\n",
        "\n",
        "##params={\n",
        "##    'objective': 'regression',\n",
        "##    'metric': 'rmse',\n",
        "##    'feature_fraction': 0.5,\n",
        "##    'learning_rate': 0.05,\n",
        "##    'max_depth': 7,\n",
        "##    'n_estimators': 800,\n",
        "##    'num_leaves': 47,\n",
        "##    'verbosity': -1,\n",
        "##    'random_state': 1,\n",
        "##    'lambda_l1': 0.9213114405513384,#92\n",
        "##    'lambda_l2': 0.4520962952151522  #45\n",
        "##}\n",
        "\n",
        "\n",
        "\n",
        "##training model for Y2 on full data\n",
        "model_final_y2.fit(X_training_full,y_training_full_Y2)\n",
        "\n",
        "##predicting on test data\n",
        "preds_y1_test=model_final_y1.predicting(X_test_full)\n",
        "preds_y2_test=model_final_y2.predicting(X_test_full)\n",
        "\n",
        "##preparing the submission DataFrame\n",
        "submission_df=pd.DataFrame({\n",
        "    'id': test['id'].astype(int),\n",
        "    'Y1': preds_y1_test.astype(float),\n",
        "    'Y2': preds_y2_test.astype(float)\n",
        "})\n",
        "\n",
        "submission_df.to_csv('submission_best_of_all.csv',index=False)\n",
        "print(\"Submission saved bro.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zqyl5ZW5F6xS",
        "outputId": "47adbcf2-c309-464d-b57d-f9a04e2d1cce"
      },
      "id": "Zqyl5ZW5F6xS",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission saved bro.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OVERALL_BEST_SUBMISSION**"
      ],
      "metadata": {
        "id": "Kcjp9f5oiOHy"
      },
      "id": "Kcjp9f5oiOHy"
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "\n",
        "features_base=list(\"ABCDEFGHIJKLMN\")+['time']\n",
        "new_features=['O','P']\n",
        "\n",
        "##combine all features for training and test\n",
        "X_training_full=pd.concat([training[features_base],training_new[new_features]],axis=1)\n",
        "y_training_full_Y1=training['Y1']\n",
        "y_training_full_Y2=training['Y2']\n",
        "X_test_full=pd.concat([test[features_base],test_new[new_features]],axis=1)\n",
        "\n",
        "##defi model for Y1 with given best params\n",
        "model_final_y1=lgb.LGBMRegressor(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    feature_fraction=0.45,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=10,\n",
        "    n_estimators=700,\n",
        "    num_leaves=60,\n",
        "    lambda_l1=0.7358897454113226,\n",
        "    lambda_l2=0.23465766293184155,\n",
        "    verbosity=-1,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "##training model for Y1 on full data\n",
        "model_final_y1.fit(X_training_full,y_training_full_Y1)\n",
        "\n",
        "##defi model for Y2 with updated params\n",
        "model_final_y2=lgb.LGBMRegressor(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    feature_fraction=0.45,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=9,\n",
        "    n_estimators=700,\n",
        "    num_leaves=92,\n",
        "    lambda_l1= 0.9213114405513384,\n",
        "    lambda_l2= 0.6520962952151522,\n",
        "    verbosity=-1,\n",
        "    random_state=202\n",
        ")\n",
        "#grid_search_parameters(with cross fold)-parameters copied: --used for deprecated\n",
        "#Final : 0.628903          Submitted - Submission 9\n",
        "##params={\n",
        "##    'objective': 'regression',\n",
        "##    'metric': 'rmse',\n",
        "##    'feature_fraction': 0.55,\n",
        "##    'learning_rate': 0.03,\n",
        "##    'max_depth': 9,\n",
        "##    'n_estimators': 700,#700\n",
        "##    'num_leaves': 92,#32-#72\n",
        "##    'verbosity': -1,\n",
        "##    'random_state': 1,\n",
        "##    'lambda_l1': 0.9213114405513384,\n",
        "##    'lambda_l2': 0.6520962952151522\n",
        "##}\n",
        "\n",
        "\n",
        "##training model for Y2 on full data\n",
        "model_final_y2.fit(X_training_full,y_training_full_Y2)\n",
        "\n",
        "##predicting on test data\n",
        "preds_y1_test=model_final_y1.predicting(X_test_full)\n",
        "preds_y2_test=model_final_y2.predicting(X_test_full)\n",
        "\n",
        "##preparing the submission DataFrame\n",
        "submission_df=pd.DataFrame({\n",
        "    'id': test['id'].astype(int),\n",
        "    'Y1': preds_y1_test.astype(float),\n",
        "    'Y2': preds_y2_test.astype(float)\n",
        "})\n",
        "\n",
        "submission_df.to_csv('submission_enhanced_22222.csv',index=False)\n",
        "print(\"Submission saved bro.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZiQpIS0LpuP",
        "outputId": "a6d86a6e-bdd3-4617-ed97-5ea6a4d7e554"
      },
      "id": "8ZiQpIS0LpuP",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission saved bro.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*EARLY STOPPING*"
      ],
      "metadata": {
        "id": "UuIIakMFy1_0"
      },
      "id": "UuIIakMFy1_0"
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import training_test_split\n",
        "\n",
        "features_base=list(\"ABCDEFGHIJKLMN\")+['time']\n",
        "new_features=['O','P']\n",
        "\n",
        "##combining the  features given\n",
        "X_full=pd.concat([training[features_base],training_new[new_features]],axis=1)\n",
        "y1_full=training['Y1']\n",
        "y2_full=training['Y2']\n",
        "\n",
        "##Create validation splits (e.g.,80% training,20% val) to monitor overfitting\n",
        "X_training1,X_val1,y_training1,y_val1=training_test_split(X_full,y1_full,test_size=0.2,random_state=1)\n",
        "X_training2,X_val2,y_training2,y_val2=training_test_split(X_full,y2_full,test_size=0.2,random_state=202)\n",
        "\n",
        "##Model params for Y1\n",
        "params_y1={\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'feature_fraction': 0.65,\n",
        "    'learning_rate': 0.01,\n",
        "    'max_depth': 10,\n",
        "    'num_leaves': 60,\n",
        "    'lambda_l1': 0.7358897454113226,\n",
        "    'lambda_l2': 0.23465766293184155,\n",
        "    'verbosity': -1,\n",
        "    'random_state': 1\n",
        "}\n",
        "\n",
        "model_y1=lgb.LGBMRegressor(**params_y1,n_estimators=10000)\n",
        "\n",
        "##training with early stopping on validation set\n",
        "model_y1.fit(\n",
        "    X_training1,y_training1,\n",
        "    eval_set=[(X_val1,y_val1)],\n",
        "    eval_metric='rmse',\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=50)],\n",
        "    #verbose=100\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "##Repeat for Y2\n",
        "params_y2={\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'feature_fraction': 0.65,\n",
        "    'learning_rate': 0.03,\n",
        "    'max_depth': 8,\n",
        "    'num_leaves': 32,\n",
        "    'lambda_l1': 0.9213114405513384,\n",
        "    'lambda_l2': 0.6520962952151522,\n",
        "    'verbosity': -1,\n",
        "    'random_state': 202\n",
        "}\n",
        "\n",
        "model_y2=lgb.LGBMRegressor(**params_y2,n_estimators=10000)\n",
        "\n",
        "model_y2.fit(\n",
        "    X_training2,y_training2,\n",
        "    eval_set=[(X_val2,y_val2)],\n",
        "    eval_metric='rmse',\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=50)],\n",
        "    #verbose=100\n",
        ")\n",
        "\n",
        "##After traininging with early stopping,use best iteration to predicting on test\n",
        "\n",
        "X_test_full=pd.concat([test[features_base],test_new[new_features]],axis=1)\n",
        "\n",
        "preds_y1_test=model_y1.predicting(X_test_full,num_iteration=model_y1.best_iteration_)\n",
        "preds_y2_test=model_y2.predicting(X_test_full,num_iteration=model_y2.best_iteration_)\n",
        "\n",
        "submission_df=pd.DataFrame({\n",
        "    'id': test['id'].astype(int),\n",
        "    'Y1': preds_y1_test.astype(float),\n",
        "    'Y2': preds_y2_test.astype(float)\n",
        "})\n",
        "\n",
        "submission_df.to_csv('submission_with_early_stopping.csv',index=False)\n",
        "print(\"Submission saved with early stopping\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-inJSgGzxySJ",
        "outputId": "5f0bae5f-5bf8-45df-ad81-a541b6d50f07"
      },
      "id": "-inJSgGzxySJ",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[669]\tvalid_0's rmse: 0.453846\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[412]\tvalid_0's rmse: 0.401177\n",
            "Submission saved with early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark=pd.read_csv('/content/submission_enhanced_with_seed_l1_tuned.csv')"
      ],
      "metadata": {
        "id": "oXQXSawW0RfP"
      },
      "id": "oXQXSawW0RfP",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "##Ensure both dataframes are sorted by 'id' before calculating R2\n",
        "submission_df_sorted=submission_df.sort_values(by='id').reset_index(drop=True)\n",
        "benchmark_sorted=benchmark.sort_values(by='id').reset_index(drop=True)\n",
        "\n",
        "##Calculate R2 for Y1 and Y2\n",
        "r2_y1=r2_score(benchmark_sorted['Y1'],submission_df_sorted['Y1'])\n",
        "r2_y2=r2_score(benchmark_sorted['Y2'],submission_df_sorted['Y2'])\n",
        "\n",
        "print(f\"R2 score for Y1: {r2_y1}\")\n",
        "print(f\"R2 score for Y2: {r2_y2}\")\n",
        "\n",
        "##Calculate and print the average R2 score\n",
        "average_r2=(r2_y1+r2_y2) / 2\n",
        "print(f\"Average R2 score: {average_r2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZFPs1Jp04R3",
        "outputId": "2a96c814-374a-40e2-eab2-ba0c3ae304a4"
      },
      "id": "UZFPs1Jp04R3",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 score for Y1: 0.9967980151253079\n",
            "R2 score for Y2: 0.9753096242689828\n",
            "Average R2 score: 0.9860538196971453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best2=pd.read_csv('/content/submission_deprecated.csv')"
      ],
      "metadata": {
        "id": "EKcxmo7z1sUy"
      },
      "id": "EKcxmo7z1sUy",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "##Ensure both dataframes are sorted by 'id' before calculating R2\n",
        "best2_sorted=best2.sort_values(by='id').reset_index(drop=True)\n",
        "benchmark_sorted=benchmark.sort_values(by='id').reset_index(drop=True)\n",
        "\n",
        "##Calculate R2 for Y1 and Y2\n",
        "r2_y1_best2=r2_score(benchmark_sorted['Y1'],best2_sorted['Y1'])\n",
        "r2_y2_best2=r2_score(benchmark_sorted['Y2'],best2_sorted['Y2'])\n",
        "\n",
        "print(f\"R2 score for Y1 (best2 vs benchmark): {r2_y1_best2}\")\n",
        "print(f\"R2 score for Y2 (best2 vs benchmark): {r2_y2_best2}\")\n",
        "\n",
        "##Calculate and print the average R2 score\n",
        "average_r2_best2=(r2_y1_best2+r2_y2_best2) / 2\n",
        "print(f\"Average R2 score (best2 vs benchmark): {average_r2_best2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19_MuX7s18tU",
        "outputId": "8f83ffdc-79f4-4c80-843c-2528271ef0f6"
      },
      "id": "19_MuX7s18tU",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 score for Y1 (best2 vs benchmark): 0.9993152644993969\n",
            "R2 score for Y2 (best2 vs benchmark): 1.0\n",
            "Average R2 score (best2 vs benchmark): 0.9996576322496984\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}