{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe0a98d4",
      "metadata": {
        "id": "fe0a98d4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hh5DVkUdTxz_",
        "outputId": "dce5aaa7-a9b6-4561-f907-ab27e5846a4d"
      },
      "id": "Hh5DVkUdTxz_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a66ece41",
      "metadata": {
        "id": "a66ece41"
      },
      "outputs": [],
      "source": [
        "train=pd.read_csv('/content/drive/MyDrive/QC/train (1).csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a0c874b",
      "metadata": {
        "id": "8a0c874b"
      },
      "outputs": [],
      "source": [
        "test=pd.read_csv('/content/drive/MyDrive/QC/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26b5d819",
      "metadata": {
        "id": "26b5d819"
      },
      "outputs": [],
      "source": [
        "train_new=pd.read_csv('/content/drive/MyDrive/QC/train_new.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c0e5857",
      "metadata": {
        "id": "7c0e5857"
      },
      "outputs": [],
      "source": [
        "test_new=pd.read_csv('/content/drive/MyDrive/QC/test_new.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection impor=in_test_split\n",
        "from sklearn.metrics import r2=ore\n",
        "import pandas as pd\n",
        "\n",
        "features_core=list(\"ABCDEFGHIJKLMN\")+['time']\n",
        "n=features=['O','P']\n",
        "\n",
        "================================================================================="
      ],
      "metadata": {
        "id": "zKxfj9Frcg6g"
      },
      "id": "zKxfj9Frcg6g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combine features\n",
        "X_combined=pd.concat([train[features_core],train_new[new_features]],axis=1)\n",
        "\n",
        "#split 80% train,20% val for both targets\n",
        "X_train,X_val,y1_train,y1_val=train_test_split(X_combined,train['Y1'],test_size=0.2,random_state=42,shuffle=True)\n",
        "_,_,y2_train,y2_val=train_test_split(X_combined,train['Y2'],test_size=0.2,random_state=42,shuffle=True)"
      ],
      "metadata": {
        "id": "gIoFmMO5_yw6"
      },
      "id": "gIoFmMO5_yw6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tune Y1 without K-Fold:**\n",
        "\n"
      ],
      "metadata": {
        "id": "XFRUAMoiW7cB"
      },
      "id": "XFRUAMoiW7cB"
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from lightgbm import early_stopping\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#defining hyperparameters in one dictionary to change easily\n",
        "params_y1={\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'feature_fraction': 0.2901089255338042, #27-done\n",
        "    'learning_rate': 0.03346736377247299, #33-done\n",
        "    'max_depth': 8, #done\n",
        "    'n_estimators': 800, #800 - done\n",
        "    'num_leaves': 54, #54-done\n",
        "    'lambda_l1': 0.8358897454113226,#83 -done\n",
        "    'lambda_l2': 0.23465766293184155,#23 -done\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "#train model with early stopping on val set\n",
        "model_y1=lgb.LGBMRegressor(**params_y1)\n",
        "\n",
        "model_y1.fit(\n",
        "    X_train,y1_train,\n",
        "    eval_set=[(X_val,y1_val)],\n",
        "    eval_metric='rmse',\n",
        "    callbacks=[early_stopping(stopping_rounds=50)],\n",
        "    #verbose=False\n",
        ")\n",
        "\n",
        "#predict\n",
        "y1_val_pred=model_y1.predict(X_val)\n",
        "val_r2_y1=r2_score(y1_val,y1_val_pred)\n",
        "print(f\"Y1 Validation R2: {val_r2_y1}\")\n",
        "\n",
        "#log results to CSV and print\n",
        "import pandas as pd\n",
        "log_path='/content/drive/MyDrive/QC/y1_res.csv'\n",
        "\n",
        "from pandas.errors import EmptyDataError\n",
        "\n",
        "try:\n",
        "    log_df=pd.read_csv('/content/drive/MyDrive/QC/y1_res.csv')\n",
        "except (FileNotFoundError,EmptyDataError):\n",
        "    #handling files not existing or empty\n",
        "    log_df=pd.DataFrame()\n",
        "\n",
        "\n",
        "\n",
        "#add current params and score\n",
        "entry=params_y1.copy()\n",
        "entry['validation_r2']=val_r2_y1\n",
        "\n",
        "log_df=pd.concat([log_df,pd.DataFrame([entry])],ignore_index=True)\n",
        "\n",
        "\n",
        "#saving updated log\n",
        "log_df.to_csv(log_path,index=False)\n",
        "\n",
        "print(\"Logged current parameters and validation R2 to y1_res.csv\")\n",
        "print(log_df.tail(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9jxUEXzhkQZ",
        "outputId": "b64eea69-8e98-47df-b046-8f4e8955377c"
      },
      "id": "V9jxUEXzhkQZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[499]\tvalid_0's rmse: 0.45012\n",
            "Y1 Validation R2: 0.7775851157118706\n",
            "Logged current parameters and validation R2 to y1_res.csv\n",
            "     objective metric  feature_fraction  learning_rate  max_depth  \\\n",
            "74  regression   rmse          0.290109       0.033467          8   \n",
            "75  regression   rmse          0.650000       0.010000         10   \n",
            "76  regression   rmse          0.290109       0.330000          8   \n",
            "77  regression   rmse          0.290109       0.033467          8   \n",
            "78  regression   rmse          0.290109       0.033467          8   \n",
            "\n",
            "    n_estimators  num_leaves  lambda_l1  lambda_l2  verbosity  random_state  \\\n",
            "74           800          54    0.83589   0.234658         -1            42   \n",
            "75           700          60    0.83589   0.234658         -1            42   \n",
            "76           800          54    0.83589   0.234658         -1            42   \n",
            "77           800          54    0.83589   0.234658         -1            42   \n",
            "78           800          54    0.83589   0.234658         -1            42   \n",
            "\n",
            "    validation_r2  \n",
            "74       0.777585  \n",
            "75       0.773438  \n",
            "76       0.766590  \n",
            "77       0.777585  \n",
            "78       0.777585  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tune Y2 without K-Fold:**"
      ],
      "metadata": {
        "id": "smgE8H2YXQ35"
      },
      "id": "smgE8H2YXQ35"
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from lightgbm import early_stopping\n",
        "from sklearn.metrics import r2_score\n",
        "import pandas as pd\n",
        "from pandas.errors import EmptyDataError\n",
        "\n",
        "#define hyperparameters for Y2\n",
        "params_y2={\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'feature_fraction': 0.4,#0.5 -done\n",
        "    'learning_rate': 0.05, #0.05 -done\n",
        "    'max_depth': 7,#done -7\n",
        "    'n_estimators': 800,#done -800\n",
        "    'num_leaves': 47,#41 - done\n",
        "    #'lambda_l1' : 0.9358897454113226,#none\n",
        "    #'lambda_l2': 0.93465766293184155,#none\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "#training with early stopping\n",
        "model_y2=lgb.LGBMRegressor(**params_y2)\n",
        "\n",
        "model_y2.fit(\n",
        "    X_train,y2_train,\n",
        "    eval_set=[(X_val,y2_val)],\n",
        "    eval_metric='rmse',\n",
        "    callbacks=[early_stopping(stopping_rounds=50)],\n",
        "    #verbose=False\n",
        ")\n",
        "\n",
        "#predict on validation set\n",
        "y2_val_pred=model_y2.predict(X_val)\n",
        "val_r2_y2=r2_score(y2_val,y2_val_pred)\n",
        "print(f\"Y2 Validation R2: {val_r2_y2}\")\n",
        "\n",
        "#log results to CSV and print\n",
        "log_path_y2='/content/drive/MyDrive/QC/y2_res.csv'\n",
        "\n",
        "try:\n",
        "    log_df_y2=pd.read_csv(log_path_y2)\n",
        "except (FileNotFoundError,EmptyDataError):\n",
        "    log_df_y2=pd.DataFrame()\n",
        "\n",
        "#add current params and score\n",
        "entry_y2=params_y2.copy()\n",
        "entry_y2['validation_r2']=val_r2_y2\n",
        "\n",
        "log_df_y2=pd.concat([log_df_y2,pd.DataFrame([entry_y2])],ignore_index=True)\n",
        "\n",
        "log_df_y2.to_csv(log_path_y2,index=False)\n",
        "\n",
        "print(\"Logged current parameters and validation R2 to y2_res.csv\")\n",
        "print(log_df_y2.tail(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyEK4lx_1jII",
        "outputId": "97b289c8-c0e5-4207-a80b-bd7660a0521a"
      },
      "id": "YyEK4lx_1jII",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[592]\tvalid_0's rmse: 0.418602\n",
            "Y2 Validation R2: 0.8033225693098163\n",
            "Logged current parameters and validation R2 to y2_res.csv\n",
            "     objective metric  feature_fraction  learning_rate  max_depth  \\\n",
            "69  regression   rmse               0.5           0.05          7   \n",
            "70  regression   rmse               0.5           0.05          7   \n",
            "71  regression   rmse               0.5           0.05          7   \n",
            "72  regression   rmse               0.5           0.05          7   \n",
            "73  regression   rmse               0.4           0.05          7   \n",
            "\n",
            "    n_estimators  num_leaves  verbosity  random_state  validation_r2  \\\n",
            "69           800          47         -1            42       0.801339   \n",
            "70           800          47         -1            42       0.802802   \n",
            "71           800          47         -1            42       0.804228   \n",
            "72           800          47         -1            42       0.806805   \n",
            "73           800          47         -1            42       0.803323   \n",
            "\n",
            "    lambda_l1  lambda_l2  \n",
            "69        NaN   0.734658  \n",
            "70        NaN   0.834658  \n",
            "71        NaN   0.934658  \n",
            "72        NaN        NaN  \n",
            "73        NaN        NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**\n",
        "\n",
        "*   The submission 7 and 8 overfitted on 20% validation data and thus they gave less R2 score compared to submission 9\n",
        "*   When submission 9's parameters are checked with that 20% validation data,I got the R2 score less than R2 score i got with submission 7 and 8,that means,i overfitted submission 7 and 8 on this 20% validation data.\n",
        "*   This 20% validation is taken with random split,but seed 42,and maintain same across all the notebook,or all the submission i made\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e2nCqWueflic"
      },
      "id": "e2nCqWueflic"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Steps:**\n",
        "*   I will check how much score does the best submission parameters - submission 9's parameters give on the 3 cross validation - the average of all 3 scores\n",
        "\n",
        "*   Then I will tune the hyperparameters starting from the submission 9's parameters ,such that they give better 3 fold validations score compared to the submission 9's 3-fold validation score\n",
        "*   List item\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZN-Zn4sWhKCl"
      },
      "id": "ZN-Zn4sWhKCl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MANUAL TUNING ON 3-FOLD FOR BEST PARAMETERS:**"
      ],
      "metadata": {
        "id": "ZM4T-NEkNwwb"
      },
      "id": "ZM4T-NEkNwwb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Y1:**"
      ],
      "metadata": {
        "id": "m2qanSH-L5m6"
      },
      "id": "m2qanSH-L5m6"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import r2_score\n",
        "from pandas.errors import EmptyDataError\n",
        "\n",
        "#define features and target for Y1\n",
        "features_base=list(\"ABCDEFGHIJKLMN\") + ['time']\n",
        "new_features=['O','P']\n",
        "X_y1=pd.concat([train[features_base],train_new[new_features]],axis=1)\n",
        "y_y1=train['Y1']\n",
        "\n",
        "#optuna_search_parameters(with op)(without cross fold):\n",
        "#Final : 0.754894-    Submitted - 0.5975\n",
        "#params={\n",
        "#    'objective': 'regression',\n",
        "#    'metric': 'rmse',\n",
        "#    'feature_fraction': 0.5524478320022036,\n",
        "#    'learning_rate': 0.019553385840773797,\n",
        "#    'max_depth': 9,\n",
        "#    'n_estimators': 1500,\n",
        "#    'num_leaves': 48,\n",
        "#    'verbosity': -1,\n",
        "#    'random_state': 42,\n",
        "#     'lambda_l1': 0.9263413222927331,\n",
        "#     'lambda_l2': 0.5416180534221455,\n",
        "#     'min_child_samples': 14,\n",
        "#     'bagging_fraction': 0.7054945561615795,\n",
        "#     'bagging_freq': 6\n",
        "#}\n",
        "\n",
        "#grid_search_parameters(with cross fold):\n",
        "##Final: 0.752910 - submitted - 0.710 - Ankur Submission ( i submitted with lamdas)\n",
        "#params={\n",
        "#    'objective': 'regression',\n",
        "#    'metric': 'rmse',\n",
        "#    'feature_fraction': 0.65,\n",
        "#    'learning_rate': 0.01,\n",
        "#    'max_depth': 10,\n",
        "#    'n_estimators': 700,\n",
        "#    'num_leaves': 60,\n",
        "#    #'lambda_l1': 0.8358897454113226,\n",
        "#    #'lambda_l2': 0.23465766293184155,\n",
        "#    'verbosity': -1,\n",
        "#    'random_state': 42\n",
        "#}\n",
        "\n",
        "\n",
        "###########################################################################################################################################\n",
        "#optuna parameters(no cross fold):\n",
        "#Final: 0.753753 - not submitted (y1_y2 file) - next this one - finetune and submit\n",
        "#params={\n",
        "#    'objective': 'regression',\n",
        "#    'metric': 'rmse',\n",
        "#    'feature_fraction': 0.5701089255338042,\n",
        "#    'learning_rate': 0.03346736377247299,\n",
        "#    'max_depth': 10,\n",
        "#    'n_estimators': 800,\n",
        "#    'num_leaves': 54,\n",
        "#    'lambda_l1': 0.8458897454113226,#53 - #84\n",
        "#    'lambda_l2': 0.43465766293184155,#43\n",
        "#    'verbosity': -1,\n",
        "#    'random_state': 42\n",
        "#}\n",
        "\n",
        "#passive/normal grid search(no cross fold) manual tuning:\n",
        "##Final:  0.747115 - submitted - submission 7 and 8\n",
        "#params={\n",
        "#    'objective': 'regression',\n",
        "#    'metric': 'rmse',\n",
        "#    'feature_fraction': 0.2901089255338042,\n",
        "#    'learning_rate': 0.03346736377247299,\n",
        "#    'max_depth': 8,\n",
        "#    'n_estimators': 800,\n",
        "#    'num_leaves': 54,\n",
        "#    'lambda_l1': 0.8358897454113226,#83 -done\n",
        "#    'lambda_l2': 0.23465766293184155,#23 -done\n",
        "#    'verbosity': -1,\n",
        "#    'random_state': 42\n",
        "#}\n",
        "\n",
        "#grid_search_parameters(with cross fold)-copied from there:\n",
        "#Final: 0.753379 - submitted 9\n",
        "params={\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'feature_fraction': 0.45,\n",
        "    'learning_rate': 0.01,\n",
        "    'max_depth': 10,\n",
        "    'n_estimators': 700,\n",
        "    'num_leaves': 60,\n",
        "    'lambda_l1': 0.735,#0.9358897454113226 #73 #8897454113226\n",
        "    'lambda_l2': 0.234,#0.23465766293184155 #23 #\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "#prepare 3-fold cross validation (shuffle=False for time-series like behavior)\n",
        "kf=KFold(n_splits=3,shuffle=False)\n",
        "\n",
        "r2_scores=[]\n",
        "\n",
        "for fold,(train_idx,val_idx) in enumerate(kf.split(X_y1)):\n",
        "    print(f\"Training fold {fold+1}\")\n",
        "    X_train_fold,X_val_fold=X_y1.iloc[train_idx],X_y1.iloc[val_idx]\n",
        "    y_train_fold,y_val_fold=y_y1.iloc[train_idx],y_y1.iloc[val_idx]\n",
        "\n",
        "    model=lgb.LGBMRegressor(**params)\n",
        "\n",
        "    model.fit(\n",
        "        X_train_fold,\n",
        "        y_train_fold,\n",
        "        eval_set=[(X_val_fold,y_val_fold)],\n",
        "        eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=50)],\n",
        "        #verbose=False\n",
        "    )\n",
        "\n",
        "    y_val_pred=model.predict(X_val_fold)\n",
        "    fold_r2=r2_score(y_val_fold,y_val_pred)\n",
        "    print(f\"Fold {fold+1} R2: {fold_r2}\")\n",
        "    r2_scores.append(fold_r2)\n",
        "\n",
        "mean_r2=sum(r2_scores) / len(r2_scores)\n",
        "print(f\"Mean R2 after 3-fold CV: {mean_r2}\")\n",
        "\n",
        "#log mean R2 and params to CSV\n",
        "log_path='/content/drive/MyDrive/QC/y1_cv_log.csv'\n",
        "\n",
        "try:\n",
        "    log_df=pd.read_csv(log_path)\n",
        "except (FileNotFoundError,EmptyDataError):\n",
        "    log_df=pd.DataFrame()\n",
        "\n",
        "#prepare dict with param values & score\n",
        "log_entry=params.copy()\n",
        "log_entry['mean_cv_r2']=mean_r2\n",
        "\n",
        "log_df=pd.concat([log_df,pd.DataFrame([log_entry])],ignore_index=True)\n",
        "log_df.to_csv(log_path,index=False)\n",
        "\n",
        "print(f\"Logged CV results to {log_path}\")\n",
        "print(log_df.tail())  #Latest entries in log\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXfpWZGcfpBR",
        "outputId": "5e4569a2-03ca-4019-8b86-fa9bfaef0f1b"
      },
      "id": "rXfpWZGcfpBR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 1\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[348]\tvalid_0's rmse: 0.35533\n",
            "Fold 1 R2: 0.7819649697007693\n",
            "Training fold 2\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[273]\tvalid_0's rmse: 0.470236\n",
            "Fold 2 R2: 0.7477670493733117\n",
            "Training fold 3\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[645]\tvalid_0's rmse: 0.605082\n",
            "Fold 3 R2: 0.7328815620863225\n",
            "Mean R2 after 3-fold CV: 0.7542045270534677\n",
            "Logged CV results to /content/drive/MyDrive/QC/y1_cv_log.csv\n",
            "     objective metric  feature_fraction  learning_rate  max_depth  \\\n",
            "77  regression   rmse              0.55           0.01         10   \n",
            "78  regression   rmse              0.45           0.01         10   \n",
            "79  regression   rmse              0.45           0.01         10   \n",
            "80  regression   rmse              0.65           0.01          9   \n",
            "81  regression   rmse              0.45           0.01         10   \n",
            "\n",
            "    n_estimators  num_leaves  lambda_l1  lambda_l2  verbosity  random_state  \\\n",
            "77           700          60    0.73589   0.234658         -1            42   \n",
            "78           700          60    0.73589   0.234658         -1            42   \n",
            "79           700          50    0.73589   0.234658         -1            42   \n",
            "80           700          60    0.73500   0.234000         -1            42   \n",
            "81           700          60    0.73500   0.234000         -1            42   \n",
            "\n",
            "    mean_cv_r2  min_child_samples  bagging_fraction  bagging_freq  \n",
            "77    0.753880                NaN               NaN           NaN  \n",
            "78    0.754218                NaN               NaN           NaN  \n",
            "79    0.754205                NaN               NaN           NaN  \n",
            "80    0.753019                NaN               NaN           NaN  \n",
            "81    0.754205                NaN               NaN           NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Y2:**"
      ],
      "metadata": {
        "id": "O_0VLmmW3uOA"
      },
      "id": "O_0VLmmW3uOA"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import r2_score\n",
        "from pandas.errors import EmptyDataError\n",
        "\n",
        "#define features and target for Y2,using old+new features\n",
        "features_base=list(\"ABCDEFGHIJKLMN\") + ['time']\n",
        "new_features=['O','P']\n",
        "X_y2=pd.concat([train[features_base],train_new[new_features]],axis=1)\n",
        "y_y2=train['Y2']\n",
        "\n",
        "#passive/normal grid search(no cross fold) manual tuning:\n",
        "#Final : 0.648659    Submitted - Submission 8\n",
        "#after lamda : 0.655779\n",
        "#params={\n",
        "#    'objective': 'regression',\n",
        "#    'metric': 'rmse',\n",
        "#    'feature_fraction': 0.5,\n",
        "#    'learning_rate': 0.05,\n",
        "#    'max_depth': 7,\n",
        "#    'n_estimators': 800,\n",
        "#    'num_leaves': 47,\n",
        "#    'verbosity': -1,\n",
        "#    'random_state': 42,\n",
        "#    'lambda_l1': 0.9213114405513384,#92\n",
        "#    'lambda_l2': 0.4520962952151522  #45\n",
        "#}\n",
        "\n",
        "#passive/normal grid search(no cross fold) no manual tuning:\n",
        "#Final : 0.616942       Submitted - Submission 7\n",
        "#params={\n",
        "#    'objective': 'regression',\n",
        "#    'metric': 'rmse',\n",
        "#    'feature_fraction': 0.7,\n",
        "#    'learning_rate': 0.05,\n",
        "#    'max_depth': 7,\n",
        "#    'n_estimators': 500,\n",
        "#    'num_leaves': 31,\n",
        "#    'verbosity': -1,\n",
        "#    'random_state': 42\n",
        "#}\n",
        "\n",
        "#grid_search_parameters(with cross fold)-parameters copied: --used for deprecated\n",
        "#Final : 0.628903          Submitted - Submission 9\n",
        "params={\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'feature_fraction': 0.45,\n",
        "    'learning_rate': 0.03,\n",
        "    'max_depth': 9,\n",
        "    'n_estimators': 700,#700\n",
        "    'num_leaves': 92,#32-#72\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42,\n",
        "    'lambda_l1': 0.9213114405513384,\n",
        "    'lambda_l2': 0.6520962952151522\n",
        "}\n",
        "\n",
        "##########################################################################################################################################\n",
        "\n",
        "#optuna_search_parameters(with op)(without cross fold):\n",
        "#Final : 0.635082    Submitted - 0.5975 score on leaderboard\n",
        "#params={\n",
        "#    'objective': 'regression',\n",
        "#    'metric': 'rmse',\n",
        "#    'feature_fraction': 0.6086455125133671,\n",
        "#    'learning_rate': 0.024435377516833785,\n",
        "#    'max_depth': 9,\n",
        "#    'n_estimators': 1500,\n",
        "#    'num_leaves': 43,\n",
        "#    'verbosity': -1,\n",
        "#    'random_state': 42,\n",
        "#    'lambda_l1': 0.6213114405513384,\n",
        "#    'lambda_l2': 0.6520962952151522,\n",
        "#    'min_child_samples': 13,\n",
        "#    'bagging_fraction': 0.9278603731265997,\n",
        "#    'bagging_freq': 7\n",
        "#}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Prepare 3-fold cross validation (shuffle=False to preserve time order)\n",
        "kf=KFold(n_splits=3,shuffle=False)\n",
        "\n",
        "r2_scores=[]\n",
        "\n",
        "for fold,(train_idx,val_idx) in enumerate(kf.split(X_y2)):\n",
        "    print(f\"Training fold {fold+1}\")\n",
        "    X_train_fold,X_val_fold=X_y2.iloc[train_idx],X_y2.iloc[val_idx]\n",
        "    y_train_fold,y_val_fold=y_y2.iloc[train_idx],y_y2.iloc[val_idx]\n",
        "\n",
        "    model=lgb.LGBMRegressor(**params)\n",
        "\n",
        "    model.fit(\n",
        "        X_train_fold,\n",
        "        y_train_fold,\n",
        "        eval_set=[(X_val_fold,y_val_fold)],\n",
        "        eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=50)],\n",
        "        #verbose=False\n",
        "    )\n",
        "\n",
        "    y_val_pred=model.predict(X_val_fold)\n",
        "    fold_r2=r2_score(y_val_fold,y_val_pred)\n",
        "    print(f\"Fold {fold+1} R2: {fold_r2}\")\n",
        "    r2_scores.append(fold_r2)\n",
        "\n",
        "mean_r2=sum(r2_scores) / len(r2_scores)\n",
        "print(f\"Mean R2 after 3-fold CV for Y2: {mean_r2}\")\n",
        "\n",
        "#Log mean R2 and params to CSV\n",
        "log_path='/content/drive/MyDrive/QC/y2_cv_log.csv'\n",
        "\n",
        "try:\n",
        "    log_df=pd.read_csv(log_path)\n",
        "except (FileNotFoundError,EmptyDataError):\n",
        "    log_df=pd.DataFrame()\n",
        "\n",
        "#Prepare dict with param values & score\n",
        "log_entry=params.copy()\n",
        "log_entry['mean_cv_r2']=mean_r2\n",
        "\n",
        "log_df=pd.concat([log_df,pd.DataFrame([log_entry])],ignore_index=True)\n",
        "log_df.to_csv(log_path,index=False)\n",
        "\n",
        "print(f\"Logged CV results to {log_path}\")\n",
        "print(log_df.tail())  #Latest entries in log\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibpwpSLq3zg_",
        "outputId": "1422c8ef-4a8e-4229-d283-15a21a3df8d4"
      },
      "id": "ibpwpSLq3zg_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 1\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[104]\tvalid_0's rmse: 0.51172\n",
            "Fold 1 R2: 0.6637336794201056\n",
            "Training fold 2\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[47]\tvalid_0's rmse: 0.443454\n",
            "Fold 2 R2: 0.6657167581597498\n",
            "Training fold 3\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[205]\tvalid_0's rmse: 0.643169\n",
            "Fold 3 R2: 0.6504824728996081\n",
            "Mean R2 after 3-fold CV for Y2: 0.6599776368264879\n",
            "Logged CV results to /content/drive/MyDrive/QC/y2_cv_log.csv\n",
            "      objective metric  feature_fraction  learning_rate  max_depth  \\\n",
            "102  regression   rmse              0.55           0.03          9   \n",
            "103  regression   rmse              0.45           0.03          9   \n",
            "104  regression   rmse              0.40           0.03          9   \n",
            "105  regression   rmse              0.42           0.03          9   \n",
            "106  regression   rmse              0.45           0.03          9   \n",
            "\n",
            "     n_estimators  num_leaves  verbosity  random_state  mean_cv_r2  lambda_l1  \\\n",
            "102           700          92         -1            42    0.658752   0.921311   \n",
            "103           700          92         -1            42    0.659978   0.921311   \n",
            "104           700          92         -1            42    0.658623   0.921311   \n",
            "105           700          92         -1            42    0.658623   0.921311   \n",
            "106           700          92         -1            42    0.659978   0.921311   \n",
            "\n",
            "     lambda_l2  min_child_samples  bagging_fraction  bagging_freq  \n",
            "102   0.652096                NaN               NaN           NaN  \n",
            "103   0.652096                NaN               NaN           NaN  \n",
            "104   0.652096                NaN               NaN           NaN  \n",
            "105   0.652096                NaN               NaN           NaN  \n",
            "106   0.652096                NaN               NaN           NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import r2_score\n",
        "from pandas.errors import EmptyDataError\n",
        "\n",
        "features_base=list(\"ABCDEFGHIJKLMN\") + ['time']\n",
        "new_features=['O','P']\n",
        "X_y2=pd.concat([train[features_base],train_new[new_features]],axis=1)\n",
        "y_y2=train['Y2']\n",
        "\n",
        "base_params={\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'feature_fraction': 0.45,\n",
        "    'max_depth': 9,\n",
        "    'n_estimators': 700,\n",
        "    'num_leaves': 92,\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42,\n",
        "    'lambda_l1': 0.9213114405513384,\n",
        "    'lambda_l2': 0.6520962952151522\n",
        "}\n",
        "\n",
        "kf=KFold(n_splits=3,shuffle=False)\n",
        "log_path='/content/drive/MyDrive/QC/y2_cv_log.csv'\n",
        "\n",
        "try:\n",
        "    log_df=pd.read_csv(log_path)\n",
        "except (FileNotFoundError,EmptyDataError):\n",
        "    log_df=pd.DataFrame()\n",
        "\n",
        "results=[]\n",
        "\n",
        "for lr in [0.005,0.01,0.02,0.03,0.04,0.05,0.07,0.1]:\n",
        "    params=base_params.copy()\n",
        "    params['learning_rate']=lr\n",
        "    r2_scores=[]\n",
        "    for train_idx,val_idx in kf.split(X_y2):\n",
        "        X_train_fold,X_val_fold=X_y2.iloc[train_idx],X_y2.iloc[val_idx]\n",
        "        y_train_fold,y_val_fold=y_y2.iloc[train_idx],y_y2.iloc[val_idx]\n",
        "        model=lgb.LGBMRegressor(**params)\n",
        "        model.fit(\n",
        "            X_train_fold,\n",
        "            y_train_fold,\n",
        "            eval_set=[(X_val_fold,y_val_fold)],\n",
        "            eval_metric='rmse',\n",
        "            callbacks=[lgb.early_stopping(stopping_rounds=50)],\n",
        "        )\n",
        "        y_val_pred=model.predict(X_val_fold)\n",
        "        r2_scores.append(r2_score(y_val_fold,y_val_pred))\n",
        "    mean_r2=sum(r2_scores) / len(r2_scores)\n",
        "    log_entry=params.copy()\n",
        "    log_entry['mean_cv_r2']=mean_r2\n",
        "    results.append(log_entry)\n",
        "\n",
        "log_df=pd.concat([log_df,pd.DataFrame(results)],ignore_index=True)\n",
        "log_df.to_csv(log_path,index=False)\n",
        "\n",
        "best_entry=max(results,key=lambda x: x['mean_cv_r2'])\n",
        "print(f\"Best learning rate: {best_entry['learning_rate']} with mean R2={best_entry['mean_cv_r2']}\")\n",
        "print(log_df.tail())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeuSJ1QPHbjQ",
        "outputId": "db25fbfc-5f44-4fd0-c003-69054b53dcac"
      },
      "id": "GeuSJ1QPHbjQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[458]\tvalid_0's rmse: 0.524316\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[285]\tvalid_0's rmse: 0.452472\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[699]\tvalid_0's rmse: 0.656654\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[238]\tvalid_0's rmse: 0.522455\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[136]\tvalid_0's rmse: 0.450373\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[634]\tvalid_0's rmse: 0.64237\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[157]\tvalid_0's rmse: 0.508741\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[70]\tvalid_0's rmse: 0.447881\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[407]\tvalid_0's rmse: 0.640836\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[104]\tvalid_0's rmse: 0.51172\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[47]\tvalid_0's rmse: 0.443454\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[205]\tvalid_0's rmse: 0.643169\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[74]\tvalid_0's rmse: 0.511543\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[32]\tvalid_0's rmse: 0.44278\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[216]\tvalid_0's rmse: 0.646765\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[62]\tvalid_0's rmse: 0.518842\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[29]\tvalid_0's rmse: 0.439476\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[154]\tvalid_0's rmse: 0.655585\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[46]\tvalid_0's rmse: 0.520521\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[18]\tvalid_0's rmse: 0.439826\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[171]\tvalid_0's rmse: 0.644394\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[21]\tvalid_0's rmse: 0.526015\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[14]\tvalid_0's rmse: 0.436847\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[79]\tvalid_0's rmse: 0.647897\n",
            "Best learning rate: 0.03 with mean R2 = 0.6599776368264879\n",
            "      objective metric  feature_fraction  learning_rate  max_depth  \\\n",
            "110  regression   rmse              0.45           0.03          9   \n",
            "111  regression   rmse              0.45           0.04          9   \n",
            "112  regression   rmse              0.45           0.05          9   \n",
            "113  regression   rmse              0.45           0.07          9   \n",
            "114  regression   rmse              0.45           0.10          9   \n",
            "\n",
            "     n_estimators  num_leaves  verbosity  random_state  mean_cv_r2  lambda_l1  \\\n",
            "110           700          92         -1            42    0.659978   0.921311   \n",
            "111           700          92         -1            42    0.659087   0.921311   \n",
            "112           700          92         -1            42    0.654284   0.921311   \n",
            "113           700          92         -1            42    0.657460   0.921311   \n",
            "114           700          92         -1            42    0.655204   0.921311   \n",
            "\n",
            "     lambda_l2  min_child_samples  bagging_fraction  bagging_freq  \n",
            "110   0.652096                NaN               NaN           NaN  \n",
            "111   0.652096                NaN               NaN           NaN  \n",
            "112   0.652096                NaN               NaN           NaN  \n",
            "113   0.652096                NaN               NaN           NaN  \n",
            "114   0.652096                NaN               NaN           NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONCLUSION is DONT USE:\n",
        "'min_child_samples': 14,\n",
        "'bagging_fraction': 0.7054945561615795,\n",
        "'bagging_freq': 6\n"
      ],
      "metadata": {
        "id": "-vidINnZHqym"
      },
      "id": "-vidINnZHqym"
    },
    {
      "cell_type": "code",
      "source": [
        "#NEW SUBMISSION\n",
        "    #'lambda_l1': 0.9213114405513384,\n",
        "    #'lambda_l2': 0.6520962952151522"
      ],
      "metadata": {
        "id": "jSHMESZ3jhNC"
      },
      "id": "jSHMESZ3jhNC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION FINAL**"
      ],
      "metadata": {
        "id": "yTGVKhk08MOZ"
      },
      "id": "yTGVKhk08MOZ"
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "\n",
        "features_base=list(\"ABCDEFGHIJKLMN\") + ['time']\n",
        "new_features=['O','P']\n",
        "\n",
        "#Combine all features for train and test\n",
        "X_train_full=pd.concat([train[features_base],train_new[new_features]],axis=1)\n",
        "y_train_full_Y1=train['Y1']\n",
        "y_train_full_Y2=train['Y2']\n",
        "X_test_full=pd.concat([test[features_base],test_new[new_features]],axis=1)\n",
        "\n",
        "#define model for Y1 with given best params\n",
        "model_final_y1=lgb.LGBMRegressor(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    feature_fraction=0.65,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=10,\n",
        "    n_estimators=700,\n",
        "    num_leaves=60,\n",
        "    lambda_l1=0.7358897454113226,\n",
        "    lambda_l2=0.23465766293184155,\n",
        "    verbosity=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "#Train model for Y1 on full data\n",
        "model_final_y1.fit(X_train_full,y_train_full_Y1)\n",
        "\n",
        "#Define model for Y2 with updated params\n",
        "model_final_y2=lgb.LGBMRegressor(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    feature_fraction=0.65,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=8,\n",
        "    n_estimators=700,\n",
        "    num_leaves=32,\n",
        "    lambda_l1= 0.9213114405513384,\n",
        "    lambda_l2= 0.6520962952151522,\n",
        "    verbosity=-1,\n",
        "    random_state=202\n",
        ")\n",
        "#grid_search_parameters(with cross fold)-parameters copied: --used for deprecated\n",
        "#Final : 0.628903          Submitted - Submission 9\n",
        "#params={\n",
        "#    'objective': 'regression',\n",
        "#    'metric': 'rmse',\n",
        "#    'feature_fraction': 0.65,\n",
        "#    'learning_rate': 0.03,\n",
        "#    'max_depth': 8,\n",
        "#    'n_estimators': 700,\n",
        "#    'num_leaves': 32,\n",
        "#    'verbosity': -1,\n",
        "#    'random_state': 42,\n",
        "#    'lambda_l1': 0.9213114405513384,\n",
        "#    'lambda_l2': 0.6520962952151522\n",
        "#}\n",
        "\n",
        "\n",
        "#Train model for Y2 on full data\n",
        "model_final_y2.fit(X_train_full,y_train_full_Y2)\n",
        "\n",
        "#Predict on test data\n",
        "preds_y1_test=model_final_y1.predict(X_test_full)\n",
        "preds_y2_test=model_final_y2.predict(X_test_full)\n",
        "\n",
        "#Prepare submission DataFrame\n",
        "submission_df=pd.DataFrame({\n",
        "    'id': test['id'].astype(int),\n",
        "    'Y1': preds_y1_test.astype(float),\n",
        "    'Y2': preds_y2_test.astype(float)\n",
        "})\n",
        "\n",
        "submission_df.to_csv('submission_enhanced_with_seed_l1_tuned.csv',index=False)\n",
        "print(\"Submission saved bro.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmuEj5mAjzZI",
        "outputId": "7a445cb8-a306-47da-96b8-028e5416c77d"
      },
      "id": "MmuEj5mAjzZI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission saved bro.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark=pd.read_csv('/content/submission_enhanced_with_seed_l1_tuned.csv')"
      ],
      "metadata": {
        "id": "oXQXSawW0RfP"
      },
      "id": "oXQXSawW0RfP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thanks for your time!**"
      ],
      "metadata": {
        "id": "_JaQg7HnABBn"
      },
      "id": "_JaQg7HnABBn"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}